{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "spec_num = 5\n",
    "use_gpu = True\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Learning Policy for Spec 5 ****\n",
      "\n",
      "**** Abstract Graph ****\n",
      "0 -> 1\n",
      "1 -> 2\n",
      "2 -> 3\n",
      "3 -> 4 6\n",
      "4 -> 5\n",
      "5 -> 5\n",
      "6 -> 7\n",
      "7 -> 7\n",
      "\n",
      "Learning policy for edge 0 -> 1\n",
      "\n",
      "Reward at episode 0: -8.018469405279378\n",
      "Reward at episode 1: -9.058039186753714\n",
      "Reward at episode 2: -6.796030016480376\n",
      "Reward at episode 3: -5.793903651200842\n",
      "Reward at episode 4: -9.19916097295803\n",
      "Expected reward after 4 episodes: -5.2136025453803425\n",
      "Reward at episode 5: -3.8324557218346627\n",
      "Reward at episode 6: -4.016335638098328\n",
      "Reward at episode 7: -10.035480542201189\n",
      "Reward at episode 8: -6.935903775359995\n",
      "Reward at episode 9: -6.814611270368831\n",
      "Expected reward after 9 episodes: -6.051809700544905\n",
      "Reward at episode 10: -7.2964770130669825\n",
      "Reward at episode 11: -8.447992200664732\n",
      "Reward at episode 12: -5.56869825214837\n",
      "Reward at episode 13: -6.044551477831004\n",
      "Reward at episode 14: -2.5754195884407034\n",
      "Expected reward after 14 episodes: -6.492531776434258\n",
      "Reward at episode 15: -2.815057472459059\n",
      "Reward at episode 16: -6.791958211001989\n",
      "Reward at episode 17: -6.822668202486297\n",
      "Reward at episode 18: -4.904197947574213\n",
      "Reward at episode 19: -10.285829798727704\n",
      "Expected reward after 19 episodes: -5.757014551612082\n",
      "Reward at episode 20: -8.148216499992534\n",
      "Reward at episode 21: -6.1890778010380885\n",
      "Reward at episode 22: -8.144056434455624\n",
      "Reward at episode 23: -11.420543411126852\n",
      "\n",
      "Critic Loss 1: 0.005406242954777553\n",
      "Critic Loss 2: 0.003946832338115201\n",
      "Actor Loss: 0.27170637875795367\n",
      "Exploration Noise: 0.1495500000000037\n",
      "\n",
      "Reward at episode 24: -9.309299753566313\n",
      "Expected reward after 24 episodes: -18.116634948776692\n",
      "Reward at episode 25: -16.609248362880667\n",
      "Reward at episode 26: -25.183812886834655\n",
      "Reward at episode 27: -14.768209071724494\n",
      "Reward at episode 28: -13.008105109090403\n",
      "Reward at episode 29: -13.855897563156956\n",
      "Expected reward after 29 episodes: -14.024827529382165\n",
      "Reward at episode 30: -17.50006289133155\n",
      "Reward at episode 31: -8.577587062527398\n",
      "Reward at episode 32: -5.05945949012829\n",
      "Reward at episode 33: -3.8722520421017443\n",
      "Reward at episode 34: -0.7451428239428841\n",
      "Expected reward after 34 episodes: -2.1415955828490794\n",
      "Reward at episode 35: -2.5970790033714146\n",
      "Reward at episode 36: -3.2174676120450356\n",
      "Reward at episode 37: -3.841228303500348\n",
      "Reward at episode 38: -3.44647012210694\n",
      "Reward at episode 39: -2.0338518970128754\n",
      "Expected reward after 39 episodes: -3.1738990536300067\n",
      "Reward at episode 40: -0.2088843456107899\n",
      "Reward at episode 41: -2.194185578885365\n",
      "Reward at episode 42: -3.1131489069594487\n",
      "Reward at episode 43: -2.6972042645714076\n",
      "Reward at episode 44: -1.7660764316791462\n",
      "Expected reward after 44 episodes: -4.29451735167684\n",
      "Reward at episode 45: -3.8361415151674403\n",
      "Reward at episode 46: -10.679491567346515\n",
      "Reward at episode 47: -2.8137956472116206\n",
      "Reward at episode 48: -2.060660618926766\n",
      "Reward at episode 49: -0.21144303815451987\n",
      "Expected reward after 49 episodes: -3.8077137886496857\n",
      "Reward at episode 50: -0.20849142140721785\n",
      "Reward at episode 51: -10.368197872121042\n",
      "\n",
      "Critic Loss 1: 0.003447110011475161\n",
      "Critic Loss 2: 0.003056776105077006\n",
      "Actor Loss: 0.464875545501709\n",
      "Exploration Noise: 0.14910000000000742\n",
      "\n",
      "Reward at episode 52: -0.9374241067405309\n",
      "Reward at episode 53: -2.960673211428806\n",
      "Reward at episode 54: -3.3207309969486194\n",
      "Expected reward after 54 episodes: -2.4703726079547934\n",
      "Reward at episode 55: -2.4472153202980054\n",
      "Reward at episode 56: -0.15721636225580787\n",
      "Reward at episode 57: -0.13306663049179404\n",
      "Reward at episode 58: -3.673261426195972\n",
      "Reward at episode 59: -1.9118502056700892\n",
      "Expected reward after 59 episodes: -4.86071016330529\n",
      "Reward at episode 60: -2.032076945259866\n",
      "Reward at episode 61: -3.639217314914482\n",
      "Reward at episode 62: -1.3767659736057782\n",
      "Reward at episode 63: -1.9305079376104746\n",
      "Reward at episode 64: -2.1899760301193423\n",
      "Expected reward after 64 episodes: -3.7277106328354983\n",
      "Reward at episode 65: -2.049147506779957\n",
      "Reward at episode 66: -4.273675194816935\n",
      "Reward at episode 67: -1.972450326959526\n",
      "Reward at episode 68: -2.009158446783774\n",
      "Reward at episode 69: -2.8851473609712492\n",
      "Expected reward after 69 episodes: -4.248810863520247\n",
      "Reward at episode 70: -4.117090570229899\n",
      "Reward at episode 71: -3.7171073224925175\n",
      "Reward at episode 72: -10.566245720640097\n",
      "Reward at episode 73: -5.988946646807807\n",
      "Reward at episode 74: -5.342314622177015\n",
      "Expected reward after 74 episodes: -2.7311837829137824\n",
      "Reward at episode 75: -3.9476289824101625\n",
      "Reward at episode 76: -3.265767176585612\n",
      "Reward at episode 77: -3.829870011556933\n",
      "\n",
      "Critic Loss 1: 0.0044844317936804145\n",
      "Critic Loss 2: 0.004354664244456217\n",
      "Actor Loss: 0.49622480392456053\n",
      "Exploration Noise: 0.14865000000001113\n",
      "\n",
      "Reward at episode 78: -2.270864986771244\n",
      "Reward at episode 79: -4.065714869099669\n",
      "Expected reward after 79 episodes: -4.217907143078962\n",
      "Reward at episode 80: -1.7175094379143947\n",
      "Reward at episode 81: -8.69951274361558\n",
      "Reward at episode 82: -8.911763952003746\n",
      "Reward at episode 83: -3.023718775341365\n",
      "Reward at episode 84: -0.07189851237031862\n",
      "Expected reward after 84 episodes: -3.810746250598959\n",
      "Reward at episode 85: -6.481352473899485\n",
      "Reward at episode 86: -2.279405714191771\n",
      "Reward at episode 87: -4.95404246321768\n",
      "Reward at episode 88: -3.0629775163201134\n",
      "Reward at episode 89: -2.899109938980383\n",
      "Expected reward after 89 episodes: -1.9443978209701522\n",
      "Reward at episode 90: -8.540157553568\n",
      "Reward at episode 91: -0.26450280022049455\n",
      "Reward at episode 92: -5.111676604557261\n",
      "Reward at episode 93: -4.8074000139319315\n",
      "Reward at episode 94: -4.158396654678707\n",
      "Expected reward after 94 episodes: -2.0377541654746496\n",
      "Reward at episode 95: -1.5654788835836138\n",
      "Reward at episode 96: -6.23471883401059\n",
      "Reward at episode 97: -3.1597121385512295\n",
      "Reward at episode 98: -2.2497193370703275\n",
      "Reward at episode 99: -1.2871313923653322\n",
      "Expected reward after 99 episodes: -4.492099652913804\n",
      "Reward at episode 100: -2.7497626954839736\n",
      "Reward at episode 101: -3.9725052417348112\n",
      "Reward at episode 102: -1.768827623043365\n",
      "Reward at episode 103: -4.568463715693768\n",
      "\n",
      "Critic Loss 1: 0.0049037171457894145\n",
      "Critic Loss 2: 0.004816478008870036\n",
      "Actor Loss: 0.5291985923051834\n",
      "Exploration Noise: 0.14820000000001485\n",
      "\n",
      "Reward at episode 104: -2.148996926240322\n",
      "Expected reward after 104 episodes: -2.925655857348211\n",
      "Reward at episode 105: -12.296443812596145\n",
      "Reward at episode 106: -0.1764784852196717\n",
      "Reward at episode 107: -2.0062521617661506\n",
      "Reward at episode 108: -3.44891732931446\n",
      "Reward at episode 109: -8.543041490586681\n",
      "Expected reward after 109 episodes: -4.891344654046418\n",
      "Reward at episode 110: -0.2326924470274675\n",
      "Reward at episode 111: -1.9201735775359752\n",
      "Reward at episode 112: -10.33991628092868\n",
      "Reward at episode 113: -1.7380661173073892\n",
      "Reward at episode 114: -0.3953134371362189\n",
      "Expected reward after 114 episodes: -2.2651867711178584\n",
      "Reward at episode 115: -2.109972575375321\n",
      "Reward at episode 116: -0.060983047255622466\n",
      "Reward at episode 117: -2.9247380036679433\n",
      "Reward at episode 118: -8.862099885705982\n",
      "Reward at episode 119: -4.603238905943403\n",
      "Expected reward after 119 episodes: -3.0211930448575997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m abstract_reach\u001b[38;5;241m.\u001b[39mpretty_print()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Step 5: Learn policy\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m path_policies \u001b[38;5;241m=\u001b[39m \u001b[43mabstract_reach\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_all_paths\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mres_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspec_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m adj_list \u001b[38;5;241m=\u001b[39m adj_list_from_task_graph(abstract_reach\u001b[38;5;241m.\u001b[39mabstract_graph)\n\u001b[1;32m    133\u001b[0m terminal_vertices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(adj_list)) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m adj_list[i]]\n",
      "File \u001b[0;32m~/dirl/spectrl/hierarchy/reachability.py:263\u001b[0m, in \u001b[0;36mAbstractReachability.learn_all_paths\u001b[0;34m(self, env, hyperparams, algo, res_model, max_steps, safety_penalty, neg_inf, alpha, num_samples, use_gpu, render, succ_thresh)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    261\u001b[0m start_dist \u001b[38;5;241m=\u001b[39m pp\u001b[38;5;241m.\u001b[39mstart_dist\n\u001b[0;32m--> 263\u001b[0m edge_policy, reach_env, log_info \u001b[38;5;241m=\u001b[39m \u001b[43medge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m final_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n",
      "File \u001b[0;32m~/dirl/spectrl/hierarchy/reachability.py:64\u001b[0m, in \u001b[0;36mAbstractEdge.learn_policy\u001b[0;34m(self, env, hyperparams, source_vertex, init_dist, algo, res_model, max_steps, safety_penalty, neg_inf, alpha, use_gpu, render)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddpg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     63\u001b[0m     agent \u001b[38;5;241m=\u001b[39m DDPG(hyperparams, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu)\n\u001b[0;32m---> 64\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreach_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     policy \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_policy()\n\u001b[1;32m     66\u001b[0m     log_info \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrewardgraph\n",
      "File \u001b[0;32m~/dirl/spectrl/rl/ddpg/ddpg.py:231\u001b[0m, in \u001b[0;36mDDPG.train\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    229\u001b[0m qPredBatch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(curStateBatch, actionBatch), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    230\u001b[0m qPredBatch2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic2(curStateBatch, actionBatch), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 231\u001b[0m qTargetBatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetQTarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextStateBatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewardBatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminalBatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Critic update\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriticOptim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/dirl/spectrl/rl/ddpg/ddpg.py:138\u001b[0m, in \u001b[0;36mDDPG.getQTarget\u001b[0;34m(self, nextStateBatch, rewardBatch, terminalBatch)\u001b[0m\n\u001b[1;32m    135\u001b[0m     targetBatch \u001b[38;5;241m=\u001b[39m targetBatch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# get actions from targetActor\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m nextActionBatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetActor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextStateBatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m target_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mtarget_noise \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m    140\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mminibatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39maction_dim)\n\u001b[1;32m    141\u001b[0m target_noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(target_noise, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mtarget_clip, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mtarget_clip)\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dirl/spectrl/rl/ddpg/actorcritic.py:48\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 48\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(state))\n\u001b[1;32m     50\u001b[0m     h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(h1))\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dirl/venv/lib/python3.8/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from conformal.all_paths_conformal_pred import all_paths_conformal_pred\n",
    "from conformal.bucketed_conformal_pred import bucketed_conformal_pred\n",
    "from conformal.nonconformity_score_graph import DIRLCumRewardScoreGraph, DIRLTimeTakenScoreGraph\n",
    "from spectrl.hierarchy.construction import adj_list_from_task_graph, automaton_graph_from_spec\n",
    "from spectrl.hierarchy.reachability import HierarchicalPolicy, ConstrainedEnv\n",
    "from spectrl.main.spec_compiler import ev, seq, choose, alw\n",
    "from spectrl.util.io import parse_command_line_options, save_log_info, save_object\n",
    "from spectrl.util.rl import print_performance, get_rollout, ObservationWrapper\n",
    "from spectrl.rl.ars import HyperParams\n",
    "from spectrl.rl.ddpg import DDPGParams\n",
    "from spectrl.envs.fetch import FetchPickAndPlaceEnv\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from spectrl.examples.rooms_envs import (\n",
    "    GRID_PARAMS_LIST,\n",
    "    MAX_TIMESTEPS,\n",
    "    START_ROOM,\n",
    "    FINAL_ROOM,\n",
    ")\n",
    "from spectrl.envs.rooms import RoomsEnv\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "\n",
    "render = False\n",
    "folder = ''\n",
    "itno = -1\n",
    "\n",
    "log_info = []\n",
    "\n",
    "def grip_near_object(err):\n",
    "    def predicate(sys_state, res_state):\n",
    "        dist = sys_state[:3] - (sys_state[3:6] + np.array([0., 0., 0.065]))\n",
    "        dist = np.concatenate([dist, [sys_state[9] + sys_state[10] - 0.1]])\n",
    "        return -LA.norm(dist) + err\n",
    "    return predicate\n",
    "\n",
    "\n",
    "def hold_object(err):\n",
    "    def predicate(sys_state, res_state):\n",
    "        dist = sys_state[:3] - sys_state[3:6]\n",
    "        dist2 = np.concatenate([dist, [sys_state[9] + sys_state[10] - 0.045]])\n",
    "        return -LA.norm(dist2) + err\n",
    "    return predicate\n",
    "\n",
    "\n",
    "def object_in_air(sys_state, res_state):\n",
    "    return sys_state[5] - 0.45\n",
    "\n",
    "\n",
    "def object_at_goal(err):\n",
    "    def predicate(sys_state, res_state):\n",
    "        dist = np.concatenate([sys_state[-3:], [sys_state[9] + sys_state[10] - 0.045]])\n",
    "        return -LA.norm(dist) + err\n",
    "    return predicate\n",
    "\n",
    "\n",
    "def gripper_reach(goal, err):\n",
    "    '''\n",
    "    goal: numpy array of dim (3,)\n",
    "    '''\n",
    "    def predicate(sys_state, res_state):\n",
    "        return -LA.norm(sys_state[:3] - goal) + err\n",
    "    return predicate\n",
    "\n",
    "\n",
    "def object_reach(goal, err):\n",
    "    '''\n",
    "    goal: numpy array of dim (3,)\n",
    "    '''\n",
    "    def predicate(sys_state, res_state):\n",
    "        return -LA.norm(sys_state[3:6] - goal) + err\n",
    "    return predicate\n",
    "\n",
    "\n",
    "above_corner1 = np.array([1.15, 1.0, 0.5])\n",
    "above_corner2 = np.array([1.45, 1.0, 0.5])\n",
    "corner1 = np.array([1.15, 1.0, 0.425])\n",
    "corner2 = np.array([1.50, 1.05, 0.425])\n",
    "\n",
    "# Specifications\n",
    "spec1 = ev(grip_near_object(0.03))\n",
    "spec2 = seq(spec1, ev(hold_object(0.03)))\n",
    "spec3 = seq(spec2, ev(object_at_goal(0.05)))\n",
    "spec4 = seq(seq(spec2, ev(object_in_air)), ev(object_at_goal(0.05)))\n",
    "spec5 = seq(seq(spec2, ev(object_in_air)), ev(object_reach(above_corner1, 0.05)))\n",
    "spec6 = seq(seq(spec2, ev(object_in_air)),\n",
    "            choose(seq(ev(object_reach(above_corner1, 0.05)), ev(object_reach(corner1, 0.05))),\n",
    "                   seq(ev(object_reach(above_corner2, 0.05)), ev(object_reach(corner2, 0.01)))))\n",
    "\n",
    "specs = [spec1, spec2, spec3, spec4, spec5, spec6]\n",
    "\n",
    "lb = [100., 100., 100., 100., 100., 100.]\n",
    "\n",
    "env = ObservationWrapper(FetchPickAndPlaceEnv(), ['observation', 'desired_goal'],\n",
    "                            relative=(('desired_goal', 0, 3), ('observation', 3, 6)))\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high\n",
    "hyperparams = DDPGParams(state_dim, action_dim, action_bound,\n",
    "                            minibatch_size=256, num_episodes=num_iters,\n",
    "                            discount=0.95, actor_hidden_dim=256,\n",
    "                            critic_hidden_dim=256, epsilon_decay=3e-6,\n",
    "                            decay_function='linear', steps_per_update=100,\n",
    "                            gradients_per_update=100, buffer_size=200000,\n",
    "                            sigma=0.15, epsilon_min=0.3, target_noise=0.0003,\n",
    "                            target_clip=0.003, warmup=1000)\n",
    "\n",
    "print('\\n**** Learning Policy for Spec {} ****'.format(spec_num))\n",
    "\n",
    "_, abstract_reach = automaton_graph_from_spec(specs[spec_num])\n",
    "print('\\n**** Abstract Graph ****')\n",
    "abstract_reach.pretty_print()\n",
    "\n",
    "# Step 5: Learn policy\n",
    "path_policies = abstract_reach.learn_all_paths(\n",
    "    env,\n",
    "    hyperparams,\n",
    "    res_model=None,\n",
    "    max_steps=40,\n",
    "    render=render,\n",
    "    neg_inf=-lb[spec_num],\n",
    "    safety_penalty=-1,\n",
    "    num_samples=1000,\n",
    "    algo=\"ddpg\",\n",
    "    alpha=0,\n",
    ")\n",
    "\n",
    "adj_list = adj_list_from_task_graph(abstract_reach.abstract_graph)\n",
    "terminal_vertices = [i for i in range(len(adj_list)) if i in adj_list[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken_score_graph = DIRLTimeTakenScoreGraph(adj_list, path_policies)\n",
    "n_samples = 2000\n",
    "es = [0.2, 0.1, 0.05]\n",
    "total_buckets = [5, 10, 20, 25, 50, 100]\n",
    "\n",
    "data_time_taken = dict()\n",
    "data_time_taken[\"metadata\"] = {\"es\": es, \"total_buckets\": total_buckets, \"scores\": \"cum-reward\", \"env\": \"16-rooms\", \"spec\": spec_num, \"n_samples\": n_samples}\n",
    "\n",
    "for e in es:\n",
    "    e_data = dict()\n",
    "    for buckets in total_buckets:\n",
    "        bucket_data = dict()\n",
    "        vbs = bucketed_conformal_pred(time_taken_score_graph, e, buckets, n_samples)\n",
    "        min_path, min_path_scores = all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "        vb = vbs.buckets[(terminal_vertices[0], buckets)]\n",
    "\n",
    "        bucket_data[\"bucketed\"] = {\"path\": vb.path, \n",
    "                                   \"path_buckets\": vb.path_buckets, \n",
    "                                   \"path_score_quantiles\": vb.path_score_quantiles, \n",
    "                                   \"max_path_score_quantile\": max(vb.path_score_quantiles)}\n",
    "        bucket_data[\"all-paths\"] = {\"path\": min_path, \"min_path_scores\": min_path_scores, \"max_min_path_scores\": max(min_path_scores)}\n",
    "        e_data[buckets] = bucket_data\n",
    "    data_time_taken[str(e)] = e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the Python object to a JSON string\n",
    "json_data = json.dumps(data_time_taken, indent=2)\n",
    "\n",
    "# Store the JSON string in a file\n",
    "with open(\"conformal_experiments_data/fetch-spec6-time-taken.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_reward_score_graph = DIRLCumRewardScoreGraph(adj_list, path_policies)\n",
    "n_samples = 2000\n",
    "es = [0.2, 0.1, 0.05]\n",
    "total_buckets = [5, 10, 20, 25, 50, 100]\n",
    "\n",
    "data_time_taken = dict()\n",
    "data_time_taken[\"metadata\"] = {\"es\": es, \"total_buckets\": total_buckets, \"scores\": \"cum-reward\", \"env\": \"9-rooms\", \"spec\": spec_num, \"n_samples\": n_samples}\n",
    "\n",
    "for e in es:\n",
    "    e_data = dict()\n",
    "    for buckets in total_buckets:\n",
    "        bucket_data = dict()\n",
    "        vbs = bucketed_conformal_pred(cum_reward_score_graph, e, buckets, n_samples)\n",
    "        min_path, min_path_scores = all_paths_conformal_pred(cum_reward_score_graph, e, n_samples)\n",
    "        vb = vbs.buckets[(terminal_vertices[0], buckets)]\n",
    "\n",
    "        bucket_data[\"bucketed\"] = {\"path\": vb.path, \n",
    "                                   \"path_buckets\": vb.path_buckets, \n",
    "                                   \"path_score_quantiles\": vb.path_score_quantiles, \n",
    "                                   \"max_path_score_quantile\": max(vb.path_score_quantiles)}\n",
    "        bucket_data[\"all-paths\"] = {\"path\": min_path, \"min_path_scores\": min_path_scores, \"max_min_path_scores\": max(min_path_scores)}\n",
    "        e_data[buckets] = bucket_data\n",
    "    data_time_taken[str(e)] = e_data\n",
    "\n",
    "# Convert the Python object to a JSON string\n",
    "json_data = json.dumps(data_time_taken, indent=2)\n",
    "\n",
    "# Store the JSON string in a file\n",
    "with open(\"conformal_experiments_data/fetch-spec6-cum-reward.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
